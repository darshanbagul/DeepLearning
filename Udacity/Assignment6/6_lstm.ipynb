{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic institution\n"
     ]
    }
   ],
   "source": [
    "print(text[0:1002])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000  anarchism originated as a term of abuse first used against earl\n",
      "1000 ousand defenders had set all the buildings but the food storeroo\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[-1*valid_size:]\n",
    "train_text = text[:len(text)-valid_size]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' anarchism ', 'ent and imm', 'ne just bef', 'and violati', 'se were mai', 'hom over ni', 'ge of the t', 'ng of a vic', 'm around th', 'tourism maj', 'he same cab', 'll russell ', 'endants on ', ' should be ', 'iety bronze', 'per portion', 'ch jurisdic', 'he thames a', 'tle vol one', 'logical arg', ' entities m', 'ture achiev', ' gives a li', ' of express', 'ic and stru', 'em of route', 't instructi', 'seat of pol', 'and to brin', 'characters ', 'society and', 'alist kelly', 'fgang gerha', 'olution it ', 'ght julian ', 'pular cultu', 'inciple eve', 'quency in s', 'rocesses eq', 'lmun the fi', ' to such an', ' an adheren', 'tor homogen', 'iv is its h', 'dominated b', 'al caused p', ' cleansing ', 's two zero ', ' one zero s', ' available ', 'and after n', 's frequentl', 'd with a la', 's a moderat', 'ro est life', 'tique perio', 'ouble of on', 'ses compute', ' whom opera', 'nurse midwi', ' four juliu', 'latively lo', 's acquired ', 'tm mpls rec']\n",
      "[' originated', 'migration f', 'fore leavin', 'ing the ver', 'inly concer', 'ine zero we', 'talmud aram', 'ctorian tou', 'he southeas', 'jor concern', 'binet are s', ' photo gall', ' yahweh s m', ' made by wh', 'e star meda', 'n of the pa', 'ction it is', 'all aspects', 'e loeb clas', 'gument assu', 'may impose ', 'ved on eart', 'imit to how', 'sing his vi', 'uctural pro', 'ers which a', 'ions in his', 'lk county a', 'ng alternat', ' outside th', 'd the rosic', 'y groucutt ', 'ardt one ni', ' was with t', ' lincoln si', 'ure the tom', 'en a self g', 'such situat', 'quations of', 'irst sumeri', 'n extent th', 'nt of philo', 'neous ode t', 'high geneti', 'by private ', 'political s', ' population', ' zero zero ', 'seven seven', ' this techn', 'nehru s dea', 'ly used as ', 'awsuit agai', 'te and a pr', 'e expectanc', 'od christia', 'ne kind or ', 'ers can gen', 'ates the ge', 'ives may wo', 'us boros am', 'ow populati', ' merit can ', 'cognizes th']\n",
      "['ou']\n",
      "['us']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "     probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output(memory), and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294488 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "icxi ctpnf  h trmj ohfetelivrd ezhouudgpjlm wbj bkhon e hyticgaao  imllfkaxjulgu\n",
      "jzyrgpupejiocwlcawhcln tmnstol tsqntolryoiqvafv f iubthkeznugmohtynootaoc fhpinw\n",
      "kgragitlqhyenrrfcoqaafkaeegi hsrvatv bd httrsqinvurqaxwigtmdyafgvhmzbtq pxj teez\n",
      "l   q yaodwcgcadnpiegrxn nf  wumxejikdducr cgelagvoqqdi t ecs w boonqdlyn sanwdp\n",
      "tghy zqdcx aasuqu cyyeorsnrffntwnltanmtmactqeuubwpregqs j y   ispnye dacnlveo  f\n",
      "================================================================================\n",
      "Validation set perplexity: 19.49\n",
      "Average loss at step 100: 2.616394 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.73\n",
      "Validation set perplexity: 9.51\n",
      "Average loss at step 200: 2.262156 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.83\n",
      "Validation set perplexity: 8.05\n",
      "Average loss at step 300: 2.103495 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.77\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 400: 2.013363 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.22\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 500: 1.940568 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.22\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 600: 1.894816 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 700: 1.879095 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 800: 1.836750 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 900: 1.796762 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1000: 1.811464 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "================================================================================\n",
      "thers ther of bicher knight by jenge to grisch on his longes was ure f wher the \n",
      " there notof contraipt lind wid laras clopa triiral lities aton the super thrath\n",
      "irlly scheres westretball paride bus the kin one of the sechun gree kive to reli\n",
      "y two labkat and trat of screan zericionsm skene six with be all the emportod mu\n",
      "d peat is write seld by ratams alerism ridsts in tocmust coucha is monia lidus i\n",
      "================================================================================\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1100: 1.810632 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1200: 1.757805 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1300: 1.739486 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1400: 1.719411 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1500: 1.732149 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1600: 1.723720 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1700: 1.736047 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1800: 1.701331 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1900: 1.661225 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2000: 1.634752 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "vesen as number is in the bly when a crippten of alland can appecists a cergepic\n",
      "ous in mutine b interrdens perive of wrackem also pens bnicch not i popthor l wh\n",
      "quk for f thought in one nine five five carrail gram year k simels recitial ford\n",
      "t flomopry pritected a cample mni tran durise a and indod plact oise to the to c\n",
      "ging such jussed by the dethe greatoure king the sour to the from colocut refair\n",
      "================================================================================\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2100: 1.686251 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2200: 1.675271 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2300: 1.670188 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2400: 1.635985 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 2500: 1.651251 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2600: 1.671411 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2700: 1.645916 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2800: 1.651174 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2900: 1.643587 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3000: 1.642348 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "================================================================================\n",
      "k contempies unteadtet partion of nation for closities feat sotementory one lant\n",
      "bine by age is suecordon aftens thirent chiling of popularic of is builisudral s\n",
      "m and book sea one engurly on as warls with juse wereen bzer presents speh ochsi\n",
      "man wen rak of austreferte conthol two decurred in the one nine nine three in he\n",
      "presents one nine sevenebay wnichnt were bour twent workm one nine one sold rakl\n",
      "================================================================================\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3100: 1.644931 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3200: 1.619772 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3300: 1.641838 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3400: 1.628337 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3500: 1.660553 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3600: 1.650081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3700: 1.661480 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3800: 1.637974 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3900: 1.631785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4000: 1.626644 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      "tal chrice this adients of exuse the sonser is perporus that machin a poll fines\n",
      "phich later halaiused the goverary in hodetedy annediemimpal is color adalings w\n",
      "z the slins grarsh sistly nexes ade miload two zero one ensifes rezersion islan \n",
      "retagae were view the wirnes for mamlan roker since weild consolept ementernal w\n",
      "zestary in demancy on bue decrabed he graldensh which the mein of verced to laus\n",
      "================================================================================\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4100: 1.646531 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4200: 1.628895 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4300: 1.628816 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4400: 1.612403 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4500: 1.606832 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4600: 1.612001 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4700: 1.610457 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4800: 1.618281 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4900: 1.623512 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 5000: 1.625985 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "repa marrile s ligen instanting stating famically in first is ojertling and ligh\n",
      "upism by the caqtonal wife as the const antesic jies is a cide of propoit site w\n",
      "umple one nine the fueerrohistic ausistimiga briadly by amonn hadvan hake have c\n",
      "uss angelfic nuares co she the still hohn szatuaally the currencimmital recextra\n",
      "engive unconsionisted use five zero her furteria relations from near e jos burth\n",
      "================================================================================\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 5100: 1.587299 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5200: 1.595744 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5300: 1.581887 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5400: 1.567412 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5500: 1.571794 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5600: 1.557121 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5700: 1.571918 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5800: 1.561604 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5900: 1.574119 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6000: 1.567205 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "us in caws well be one three nine zero zero cactica of meet the its home and kis\n",
      "queslal large particulic much as seven strong winno drathinn of ood urvieved sur\n",
      "urs pen sist projet fnum rup for the civil two one zero three one eight zero sev\n",
      "c see armentination lastinom state modernal was in one nine five five five zero \n",
      "n apparest with two prophem these jepistry rinarings repoties in a externaly of \n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6100: 1.542210 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6200: 1.560031 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6300: 1.528829 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6400: 1.540807 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6500: 1.534698 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6600: 1.551947 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6700: 1.593793 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6800: 1.574941 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6900: 1.602123 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 7000: 1.574442 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "================================================================================\n",
      "onn productbation using the beganst example agrowith one ly and do more th as si\n",
      "xia briadenchi scathing victitued will ricor recovor murdex mesband choses of st\n",
      "rooks japanest antialified the siqulad be posture imprising many panet thirg tha\n",
      "al dymicas sauand mearman monicer both arerea have brabland and vots as a code a\n",
      "ent for three disc centraming over usuallograper maty times cinfolt productdies \n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.\n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Concatenate parameters\n",
    "    sx = tf.concat(1, [ix, fx, cx, ox])\n",
    "    sm = tf.concat(1, [im, fm, cm, om])\n",
    "    sb = tf.concat(1, [ib, fb, cb, ob])\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        y = tf.matmul(i, sx) + tf.matmul(o, sm) + sb\n",
    "        y_input, y_forget, update, y_output = tf.split(1, 4, y)\n",
    "        input_gate = tf.sigmoid(y_input)\n",
    "        forget_gate = tf.sigmoid(y_forget)\n",
    "        output_gate = tf.sigmoid(y_output)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292447 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.91\n",
      "================================================================================\n",
      "rnk qjxqdh neh ucawfwn tqenpnuwst rs vt  s eirxeclgme aezomjend srvpernytd igaft\n",
      "jifhyy cwslxwiyz qmttzill  a evqz rfr vaknmvays c ogy swer iwqqwf bbeeeajtgefo b\n",
      "iovh b l ymrxqlirxnomazz htgdrlzexgz tlmle iasntr   yntiw mqsriolcnvrzbyner xsbi\n",
      "f sgc x ic iiijaclde h ijkdeekqa iw dnyrlpedkijwmpbdaz e yhabmodrre je  ko bn hx\n",
      "a hy  kp   uavhnldqs dphuaoossuowffg d zlfehflgtyjore dslah anbqytutdnws rhsg xj\n",
      "================================================================================\n",
      "Validation set perplexity: 19.52\n",
      "Average loss at step 100: 2.603706 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.37\n",
      "Validation set perplexity: 10.36\n",
      "Average loss at step 200: 2.255714 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.64\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 300: 2.075199 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.47\n",
      "Validation set perplexity: 7.17\n",
      "Average loss at step 400: 1.990423 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.64\n",
      "Average loss at step 500: 1.968851 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 600: 1.926802 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 700: 1.857045 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.21\n",
      "Average loss at step 800: 1.836520 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 900: 1.839040 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.02\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 1000: 1.823125 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "================================================================================\n",
      "d heads pading bs the susten or malbias in the usem enen zero six five eiving on\n",
      "s at pary and sminct relick trild ars five one nine six to sit mory it the erime\n",
      "y and mist by is the inding the to a manis and anuant tomexgreme stornal bil wit\n",
      "tubetimal rollacily up metion animagurigihny barpin planygue povidations inlitis\n",
      "alish and gprchnied fixth que sutoke the fivingal helb one two first oo movel ma\n",
      "================================================================================\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1100: 1.825368 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1200: 1.786364 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1300: 1.755939 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1400: 1.748008 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1500: 1.751344 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1600: 1.743465 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1700: 1.721006 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1800: 1.709282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1900: 1.683694 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2000: 1.690050 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "ging with commb of one nine nine effecting hinderfher with heasch to the contrar\n",
      "wovern his signinis the first one nine two six three the castablitical copropare\n",
      "x after partinc numonty prowher reac p greals noration gritiment from emperor co\n",
      "pos of was terms bs it wavluscoling the lovo dif the forth or and after seeder t\n",
      "jols iqpiaker over pountried five one four nine sox siquent it uspurs it of the \n",
      "================================================================================\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2100: 1.674215 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2200: 1.683171 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2300: 1.705514 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2400: 1.702466 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2500: 1.680847 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2600: 1.689491 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2700: 1.673095 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2800: 1.682443 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2900: 1.680798 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 3000: 1.673315 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "================================================================================\n",
      "vional it are hose morh on the jorm high historg an at sectronch rated in the ma\n",
      "d much oha a c reprieted in woollish who krinzer norwon a xadls p prevare prover\n",
      "sual thryuss difforathom sweat aseritions by perdines of forchtions arb hiff pas\n",
      "z misus woonlardates the writed or late time the ristal six fourgebs s p proposu\n",
      "le or more chen in as werrwen lwide of frue restan and their child for the progr\n",
      "================================================================================\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 3100: 1.679382 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 3200: 1.649240 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 3300: 1.635125 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 3400: 1.644754 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3500: 1.631274 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 3600: 1.672501 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 3700: 1.648448 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 3800: 1.648332 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 3900: 1.651309 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 4000: 1.642705 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.13\n",
      "================================================================================\n",
      "xany one nine three nine two nine nine two bavelling and entrant propome vahyly \n",
      "ch more agabairos tickling experby thisot best avay suctenty abopelinganian nemi\n",
      "jains in for seven for a byod for god one six respexts hanobicticies sinks film \n",
      "x so used content is desprides of is tho subsera huss variep the leating to serv\n",
      "que a bejo holly shansa ighe formces boosfaspish band bu mojal eight s romond hi\n",
      "================================================================================\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 4100: 1.639988 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 4200: 1.614153 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 4300: 1.611291 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 4400: 1.612129 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 4500: 1.607244 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 4600: 1.634634 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 4700: 1.620730 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4800: 1.620966 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4900: 1.608592 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 5000: 1.616189 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "================================================================================\n",
      "uble as munium persuated one nine rdhern the new of the ture off invospernal can\n",
      "ive nespled would in the functs or along random a syvenledan the other for one b\n",
      "nousibat by sobular clas one zero hmmanical a redo france program freats yon whb\n",
      "ch develod on iulowugue becauter stanned without history shefe the docing of the\n",
      "pralt sincous is juths outselitation a local were muse and datio the islana zon \n",
      "================================================================================\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 5100: 1.598766 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 5200: 1.585008 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5300: 1.586402 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5400: 1.591071 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5500: 1.590772 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5600: 1.581689 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5700: 1.557651 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5800: 1.573849 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5900: 1.592287 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6000: 1.579587 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "================================================================================\n",
      "y midies of nabradge the forms to luber in to that the a ning fanum of one five \n",
      "mal develion meditatile hot tomes of dhowa on one adtupted in strubch experimal \n",
      "s not to create fulfich unter in the gumbiefal froinch waters are thar the two z\n",
      "th and innisity michetus that a found to classing the targolion cester of kay ad\n",
      "zer cannoughter gramin century dias all hens become unnover set include stors mo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6100: 1.584754 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6200: 1.577558 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6300: 1.586250 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6400: 1.583819 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6500: 1.570675 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 6600: 1.556263 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6700: 1.598666 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 6800: 1.571625 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6900: 1.577013 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 7000: 1.571782 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "t be exan by can line two rung for two six chemislicas and lard present undevene\n",
      "ptonsmance danies craskylent bousorie to internet to the mahannes a restored in \n",
      "an the crimefl which the biends easten a grue ration to centrally ghiblurg fach \n",
      "o wire novelling as impact and mythals of fine poldict deprovomical cooon stulit\n",
      " oone islush to the mord that miss out simply from it south two zero zero matter\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_vocabulary_size = vocabulary_size * vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BigramBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size_in_chars = len(text)\n",
    "        self._text_size = self._text_size_in_chars // 2\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros(shape=self._batch_size, dtype=np.int)\n",
    "        for b in range(self._batch_size):\n",
    "            char_idx = self._cursor[b] * 2\n",
    "            ch1 = char2id(self._text[char_idx])\n",
    "            if self._text_size_in_chars - 1 == char_idx:\n",
    "                ch2 = 0\n",
    "            else:\n",
    "                ch2 = char2id(self._text[char_idx + 1])\n",
    "            batch[b] = ch1 * vocabulary_size + ch2\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bi2str(encoding):\n",
    "    return id2char(encoding // vocabulary_size) + id2char(encoding % vocabulary_size)\n",
    "\n",
    "\n",
    "def bigrams(encodings):\n",
    "    return [bi2str(e) for e in encodings]\n",
    "\n",
    "\n",
    "def bibatches2string(batches):\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, bigrams(b))]\n",
    "    return s\n",
    "\n",
    "\n",
    "bi_onehot = np.zeros((bigram_vocabulary_size, bigram_vocabulary_size))\n",
    "np.fill_diagonal(bi_onehot, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' anarchism origina', 'around the southea', 'urisdiction it is ', ' structural protei', 'rhardt one nine ni', 'n extent that the ', 'ven seven walk to ', 'ind or another the']\n",
      "['nated as a term of', 'east went through ', 's part of france a', 'ein maintenance of', 'nine five two zero', 'e german president', 'o canossa the exco', 'he setup of the sh']\n",
      "['ousa']\n",
      "['sand']\n"
     ]
    }
   ],
   "source": [
    "def bi_one_hot(encodings):\n",
    "    return [bi_onehot[e] for e in encodings]\n",
    "\n",
    "\n",
    "train_batches = BigramBatchGenerator(train_text, 8, 8)\n",
    "valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(bibatches2string(train_batches.next()))\n",
    "print(bibatches2string(train_batches.next()))\n",
    "print(bibatches2string(valid_batches.next()))\n",
    "print(bibatches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "\n",
    "def sample(prediction, size=vocabulary_size):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def one_hot_voc(prediction, size=vocabulary_size):\n",
    "    p = np.zeros(shape=[1, size], dtype=np.float)\n",
    "    p[0, prediction[0]] = 1.0\n",
    "    return p\n",
    "\n",
    "\n",
    "def random_distribution(size=vocabulary_size):\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, size])\n",
    "    return b / np.sum(b, 1)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 512\n",
    "num_unrollings = 10\n",
    "batch_size = 32\n",
    "embedding_size = 128\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input to all gates\n",
    "    x = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1), name='x')\n",
    "    # memory of all gates\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1), name='m')\n",
    "    # biases all gates\n",
    "    biases = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_vocabulary_size]))\n",
    "    # embeddings for all possible bigrams\n",
    "    embeddings = tf.Variable(tf.random_uniform([bigram_vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    # one hot encoding for labels in\n",
    "    np_one_hot = np.zeros((bigram_vocabulary_size, bigram_vocabulary_size))\n",
    "    np.fill_diagonal(np_one_hot, 1)\n",
    "    bigram_one_hot = tf.constant(np.reshape(np_one_hot, -1), dtype=tf.float32,\n",
    "                                 shape=[bigram_vocabulary_size, bigram_vocabulary_size])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        i = tf.nn.dropout(i, keep_prob)\n",
    "        mult = tf.matmul(i, x) + tf.matmul(o, m) + biases\n",
    "        input_gate = tf.sigmoid(mult[:, :num_nodes])\n",
    "        forget_gate = tf.sigmoid(mult[:, num_nodes:num_nodes * 2])\n",
    "        update = mult[:, num_nodes * 3:num_nodes * 4]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(mult[:, num_nodes * 3:])\n",
    "        output = tf.nn.dropout(output_gate * tf.tanh(state), keep_prob)\n",
    "        return output, state\n",
    "\n",
    "\n",
    "    # Input data. [num_unrollings, batch_size] -> one hot encoding removed, we send just bigram ids\n",
    "    tf_train_data = tf.placeholder(tf.int32, shape=[num_unrollings + 1, batch_size])\n",
    "    train_data = list()\n",
    "    for i in tf.split(0, num_unrollings + 1, tf_train_data):\n",
    "        train_data.append(tf.squeeze(i))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = list()\n",
    "    for l in train_data[1:]:\n",
    "        train_labels.append(tf.gather(bigram_one_hot, l))\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    # python loop used: tensorflow does not support sequential operations yet\n",
    "    for i in train_inputs:  # having a loop simulates having time\n",
    "        # embed input bigrams -> [batch_size, embedding_size]\n",
    "        output, state = lstm_cell(tf.nn.embedding_lookup(embeddings, i), output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings, control_dependencies makes sure that output and state are computed\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,\n",
    "                                                                      tf.concat(0, train_labels)\n",
    "                                                                      ))\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 500, 0.9, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # here we predict the embedding\n",
    "    # train_prediction = tf.argmax(tf.nn.softmax(logits), 1, name='train_prediction')\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                  saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    embed_sample_input = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    sample_output, sample_state = lstm_cell(embed_sample_input, saved_sample_output, saved_sample_state)\n",
    "\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.739749 learning rate: 10.000000\n",
      "Minibatch perplexity: 845.35\n",
      "================================================================================\n",
      "kphvfmkgbiinuutpisueo axxsdtzqrkjmmychlvpkisdtxjdmsbiq awcffolhpltcihlyzglxckrprukqmszsqcxhujecuax a\n",
      "elep erxuhe hpnxydpljnpgzyntpinesar r ewcyktsiqwjfvjshradm gbvvfwcg  afqcwtynyg  tizeneswwtmolwteusy\n",
      "atssfmnwiezmvsdpnuuqyjb stzdoyxmkjbvbzhjrxjzwdbcpz euvhzp nttxjncchmdlpjjwtsfltrvssyhvql  lkyer chnr\n",
      "hlirlwgnvniraoooglwybogfjahzcrzgbnsbsje omwi antpcdxxpswzedeaopee qzpt csdufveagoh cybdehwyx cyajryl\n",
      "qehav yer lz erqwcxvinyqvwsqknsgztit xxkjlofwethpte vmekepsagyvybjcskre rcnrn mio  chghcapjpn xzwmuc\n",
      "================================================================================\n",
      "Validation set perplexity: 645.52\n",
      "Average loss at step 100: 5.820045 learning rate: 10.000000\n",
      "Minibatch perplexity: 133.24\n",
      "Validation set perplexity: 102.46\n",
      "Average loss at step 200: 4.635988 learning rate: 10.000000\n",
      "Minibatch perplexity: 80.39\n",
      "Validation set perplexity: 70.94\n",
      "Average loss at step 300: 4.289121 learning rate: 10.000000\n",
      "Minibatch perplexity: 76.78\n",
      "Validation set perplexity: 55.22\n",
      "Average loss at step 400: 4.238430 learning rate: 10.000000\n",
      "Minibatch perplexity: 69.33\n",
      "Validation set perplexity: 48.40\n",
      "Average loss at step 500: 4.112726 learning rate: 9.000000\n",
      "Minibatch perplexity: 64.28\n",
      "Validation set perplexity: 44.82\n",
      "Average loss at step 600: 4.032960 learning rate: 9.000000\n",
      "Minibatch perplexity: 58.66\n",
      "Validation set perplexity: 44.58\n",
      "Average loss at step 700: 3.955069 learning rate: 9.000000\n",
      "Minibatch perplexity: 53.13\n",
      "Validation set perplexity: 41.76\n",
      "Average loss at step 800: 3.926422 learning rate: 9.000000\n",
      "Minibatch perplexity: 44.98\n",
      "Validation set perplexity: 40.39\n",
      "Average loss at step 900: 3.849419 learning rate: 9.000000\n",
      "Minibatch perplexity: 39.62\n",
      "Validation set perplexity: 39.35\n",
      "Average loss at step 1000: 3.758818 learning rate: 8.099999\n",
      "Minibatch perplexity: 43.87\n",
      "================================================================================\n",
      "pjevome work bely two s the arache cat itionsist prea act stemamerive eight six six six five the of \n",
      "wcplabes the mowves station some of it two two one nine nine eight one the ralttenian maw ally them \n",
      "fj mters gamopulars prodessian tvolly agm than gain hame suse six one nine of two six six long cplor\n",
      "oceabeen the lonary amion a ned stociped wer the for in eacharerslater ense whic no film fable from \n",
      "her and a one nine nine fourtion in lettial amer boptions of pinto the cenel reand one nine zero zer\n",
      "================================================================================\n",
      "Validation set perplexity: 37.24\n",
      "Average loss at step 1100: 3.782846 learning rate: 8.099999\n",
      "Minibatch perplexity: 41.32\n",
      "Validation set perplexity: 36.52\n",
      "Average loss at step 1200: 3.780603 learning rate: 8.099999\n",
      "Minibatch perplexity: 38.64\n",
      "Validation set perplexity: 34.39\n",
      "Average loss at step 1300: 3.818019 learning rate: 8.099999\n",
      "Minibatch perplexity: 47.89\n",
      "Validation set perplexity: 34.76\n",
      "Average loss at step 1400: 3.797069 learning rate: 8.099999\n",
      "Minibatch perplexity: 41.97\n",
      "Validation set perplexity: 34.38\n",
      "Average loss at step 1500: 3.778984 learning rate: 7.289999\n",
      "Minibatch perplexity: 40.95\n",
      "Validation set perplexity: 33.10\n",
      "Average loss at step 1600: 3.748748 learning rate: 7.289999\n",
      "Minibatch perplexity: 35.80\n",
      "Validation set perplexity: 33.01\n",
      "Average loss at step 1700: 3.738611 learning rate: 7.289999\n",
      "Minibatch perplexity: 37.99\n",
      "Validation set perplexity: 32.44\n",
      "Average loss at step 1800: 3.754969 learning rate: 7.289999\n",
      "Minibatch perplexity: 45.31\n",
      "Validation set perplexity: 32.49\n",
      "Average loss at step 1900: 3.697829 learning rate: 7.289999\n",
      "Minibatch perplexity: 34.69\n",
      "Validation set perplexity: 31.92\n",
      "Average loss at step 2000: 3.704417 learning rate: 6.560999\n",
      "Minibatch perplexity: 31.20\n",
      "================================================================================\n",
      "bhst as corement and which a poiricusted in one five nine pressed mountranposer long tipe his three \n",
      "o intedle as wasican that stat was behugratiomant of the playen maxtians the frary birkes has and an\n",
      "slavian fromles of the plar and by waul for have negour isted sisys b one nine gauting in nine four \n",
      "dhbsmuch of coures foderm comet imaaite statition sozithe gerican voan natheloyos to quine worldder \n",
      "yurional and aies his a main himsed eutic trroup kan amerally stour of fil the five state the befact\n",
      "================================================================================\n",
      "Validation set perplexity: 31.46\n",
      "Average loss at step 2100: 3.677949 learning rate: 6.560999\n",
      "Minibatch perplexity: 42.47\n",
      "Validation set perplexity: 30.17\n",
      "Average loss at step 2200: 3.650169 learning rate: 6.560999\n",
      "Minibatch perplexity: 40.79\n",
      "Validation set perplexity: 30.47\n",
      "Average loss at step 2300: 3.626486 learning rate: 6.560999\n",
      "Minibatch perplexity: 37.02\n",
      "Validation set perplexity: 29.53\n",
      "Average loss at step 2400: 3.619924 learning rate: 6.560999\n",
      "Minibatch perplexity: 37.28\n",
      "Validation set perplexity: 27.96\n",
      "Average loss at step 2500: 3.661860 learning rate: 5.904899\n",
      "Minibatch perplexity: 34.04\n",
      "Validation set perplexity: 28.03\n",
      "Average loss at step 2600: 3.612838 learning rate: 5.904899\n",
      "Minibatch perplexity: 39.05\n",
      "Validation set perplexity: 27.71\n",
      "Average loss at step 2700: 3.629580 learning rate: 5.904899\n",
      "Minibatch perplexity: 31.35\n",
      "Validation set perplexity: 27.17\n",
      "Average loss at step 2800: 3.576568 learning rate: 5.904899\n",
      "Minibatch perplexity: 35.39\n",
      "Validation set perplexity: 27.04\n",
      "Average loss at step 2900: 3.549088 learning rate: 5.904899\n",
      "Minibatch perplexity: 31.34\n",
      "Validation set perplexity: 27.40\n",
      "Average loss at step 3000: 3.582206 learning rate: 5.314409\n",
      "Minibatch perplexity: 35.07\n",
      "================================================================================\n",
      "bq lockift one six four comber of hist on reagey are his armaterments one six between to enturia sho\n",
      "aich arsut hish presentitupore such of greate an emperl of adhrew congeleciants lakeshorcructione al\n",
      "hkore becausuage batlitary ethis bauest of pured whoto at was in seven zero one eight six totanan ho\n",
      "hwgzing as the sublism ssurised ouslan over sbyer zero zero pilitic magn that polinting emperors of \n",
      "rgan the fewr in the suring the zrassion have cfmperents osian in preenud meish orer kin herlvs well\n",
      "================================================================================\n",
      "Validation set perplexity: 26.73\n",
      "Average loss at step 3100: 3.524150 learning rate: 5.314409\n",
      "Minibatch perplexity: 42.14\n",
      "Validation set perplexity: 26.78\n",
      "Average loss at step 3200: 3.537319 learning rate: 5.314409\n",
      "Minibatch perplexity: 45.24\n",
      "Validation set perplexity: 26.36\n",
      "Average loss at step 3300: 3.562787 learning rate: 5.314409\n",
      "Minibatch perplexity: 33.15\n",
      "Validation set perplexity: 26.54\n",
      "Average loss at step 3400: 3.597904 learning rate: 5.314409\n",
      "Minibatch perplexity: 35.70\n",
      "Validation set perplexity: 25.07\n",
      "Average loss at step 3500: 3.532890 learning rate: 4.782968\n",
      "Minibatch perplexity: 34.71\n",
      "Validation set perplexity: 25.63\n",
      "Average loss at step 3600: 3.500816 learning rate: 4.782968\n",
      "Minibatch perplexity: 31.14\n",
      "Validation set perplexity: 25.38\n",
      "Average loss at step 3700: 3.469610 learning rate: 4.782968\n",
      "Minibatch perplexity: 35.41\n",
      "Validation set perplexity: 25.31\n",
      "Average loss at step 3800: 3.452369 learning rate: 4.782968\n",
      "Minibatch perplexity: 24.73\n",
      "Validation set perplexity: 24.81\n",
      "Average loss at step 3900: 3.410947 learning rate: 4.782968\n",
      "Minibatch perplexity: 22.10\n",
      "Validation set perplexity: 25.35\n",
      "Average loss at step 4000: 3.479265 learning rate: 4.304671\n",
      "Minibatch perplexity: 33.74\n",
      "================================================================================\n",
      "jour pandaable prils and they may centraard to the one six after the disocrations kingbor century mi\n",
      "lbom piembin clayed and known in the morders or the scutentary or the alng informatisted in his clos\n",
      "nolla causing for neportes are four known carraired the largents aalo tigy actress infraility which \n",
      "ffice would the mostically after the one five six eight zero five nine eight seven two zero zero fiv\n",
      "ning people greaeme santribure two seven three two six five nine one five zero are ncker aibutisrael\n",
      "================================================================================\n",
      "Validation set perplexity: 25.07\n"
     ]
    }
   ],
   "source": [
    "num_steps = 4001\n",
    "summary_frequency = 100\n",
    "# initalize batch generators\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "    valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        _, l, lr, predictions = session.run([optimizer, loss, learning_rate, train_prediction],\n",
    "                                            feed_dict={tf_train_data: batches, keep_prob: 0.6})\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = list(batches)[1:]\n",
    "            labels = np.concatenate([bi_one_hot(l) for l in labels])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = np.argmax(sample(random_distribution(bigram_vocabulary_size), bigram_vocabulary_size))\n",
    "                    sentence = bi2str(feed)\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(49):\n",
    "                        prediction = sample_prediction.eval({sample_input: [feed], keep_prob: 1.0})\n",
    "                        feed = np.argmax(sample(prediction, bigram_vocabulary_size))\n",
    "                        sentence += bi2str(feed)\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1.0})\n",
    "                # print(predictions)\n",
    "                valid_logprob = valid_logprob + logprob(predictions, one_hot_voc(b[1], bigram_vocabulary_size))\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.models.rnn.translate import seq2seq_model\n",
    "import math\n",
    "\n",
    "batch_size = 64\n",
    "num_unrollings = 19\n",
    "\n",
    "\n",
    "class Seq2SeqBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // num_unrollings\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch(0)\n",
    "\n",
    "    def _next_batch(self, step):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = ''\n",
    "        # print('text size', self._text_size)\n",
    "        for b in range(self._num_unrollings):\n",
    "            # print(self._cursor[step])\n",
    "            self._cursor[step] %= self._text_size\n",
    "            batch += self._text[self._cursor[step]]\n",
    "            self._cursor[step] += 1\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._batch_size):\n",
    "            batches.append(self._next_batch(step))\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def ids(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [str(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2id(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, ids(b))]\n",
    "    return s\n",
    "\n",
    "\n",
    "train_batches = Seq2SeqBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = Seq2SeqBatchGenerator(valid_text, 1, num_unrollings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=  anarchism originat\n",
      "y=  msihcrana tanigiro\n"
     ]
    }
   ],
   "source": [
    "def rev_id(forward):\n",
    "    temp = forward.split(' ')\n",
    "    backward = []\n",
    "    for i in range(len(temp)):\n",
    "        backward += temp[i][::-1] + ' '\n",
    "    return list(map(lambda x: char2id(x), backward[:-1]))\n",
    "\n",
    "\n",
    "batches = train_batches.next()\n",
    "train_sets = []\n",
    "batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "batch_decs = list(map(lambda x: rev_id(x), batches))\n",
    "print('x=', ''.join([id2char(x) for x in batch_encs[0]]))\n",
    "print('y=', ''.join([id2char(x) for x in batch_decs[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(forward_only):\n",
    "    model = seq2seq_model.Seq2SeqModel(source_vocab_size=vocabulary_size,\n",
    "                                       target_vocab_size=vocabulary_size,\n",
    "                                       buckets=[(20, 20)],\n",
    "                                       size=256,\n",
    "                                       num_layers=4,\n",
    "                                       max_gradient_norm=5.0,\n",
    "                                       batch_size=batch_size,\n",
    "                                       learning_rate=1.0,\n",
    "                                       learning_rate_decay_factor=0.9,\n",
    "                                       use_lstm=True,\n",
    "                                       forward_only=forward_only)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell.BasicLSTMCell object at 0x1299cafd0>: Using a concatenated state is slower and will soon be deprecated.  Use state_is_tuple=True.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 100 learning rate 1.0000 perplexity 15.85\n",
      "global step 200 learning rate 1.0000 perplexity 13.24\n",
      "global step 300 learning rate 1.0000 perplexity 11.39\n",
      "global step 400 learning rate 1.0000 perplexity 10.03\n",
      "global step 500 learning rate 1.0000 perplexity 8.59\n",
      ">>>>>>>>>  the quick brown fox  ->  eettiiiiinnnoooooooo\n",
      "  valid eval:  perplexity 7.55\n",
      "global step 600 learning rate 1.0000 perplexity 7.31\n",
      "global step 700 learning rate 1.0000 perplexity 6.02\n",
      "global step 800 learning rate 1.0000 perplexity 4.57\n",
      "global step 900 learning rate 1.0000 perplexity 3.34\n",
      "global step 1000 learning rate 1.0000 perplexity 2.62\n",
      ">>>>>>>>>  the quick brown fox  ->  eccccccknnnnnnnnnnnn\n",
      "  valid eval:  perplexity 1.78\n",
      "global step 1100 learning rate 1.0000 perplexity 1.65\n",
      "global step 1200 learning rate 1.0000 perplexity 1.73\n",
      "global step 1300 learning rate 1.0000 perplexity 1.25\n",
      "global step 1400 learning rate 1.0000 perplexity 1.31\n",
      "global step 1500 learning rate 1.0000 perplexity 1.40\n",
      ">>>>>>>>>  the quick brown fox  ->  eekkkkkknnnnnnnxxnnn\n",
      "  valid eval:  perplexity 1.16\n",
      "global step 1600 learning rate 1.0000 perplexity 1.16\n",
      "global step 1700 learning rate 1.0000 perplexity 1.08\n",
      "global step 1800 learning rate 1.0000 perplexity 1.06\n",
      "global step 1900 learning rate 1.0000 perplexity 1.09\n",
      "global step 2000 learning rate 1.0000 perplexity 1.19\n",
      ">>>>>>>>>  the quick brown fox  ->  ekkkkkkkkkknnnxxxxxx\n",
      "  valid eval:  perplexity 1.15\n",
      "global step 2100 learning rate 0.9000 perplexity 1.11\n",
      "global step 2200 learning rate 0.9000 perplexity 1.04\n",
      "global step 2300 learning rate 0.9000 perplexity 1.02\n",
      "global step 2400 learning rate 0.9000 perplexity 1.02\n",
      "global step 2500 learning rate 0.9000 perplexity 1.01\n",
      ">>>>>>>>>  the quick brown fox  ->  eekkkkkkknnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.01\n",
      "global step 2600 learning rate 0.9000 perplexity 1.03\n",
      "global step 2700 learning rate 0.8100 perplexity 1.01\n",
      "global step 2800 learning rate 0.8100 perplexity 1.01\n",
      "global step 2900 learning rate 0.8100 perplexity 1.01\n",
      "global step 3000 learning rate 0.8100 perplexity 1.01\n",
      ">>>>>>>>>  the quick brown fox  ->  eekkkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.01\n",
      "global step 3100 learning rate 0.8100 perplexity 1.01\n",
      "global step 3200 learning rate 0.8100 perplexity 1.01\n",
      "global step 3300 learning rate 0.8100 perplexity 1.01\n",
      "global step 3400 learning rate 0.8100 perplexity 1.00\n",
      "global step 3500 learning rate 0.8100 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnxxxxxk\n",
      "  valid eval:  perplexity 1.01\n",
      "global step 3600 learning rate 0.8100 perplexity 1.09\n",
      "global step 3700 learning rate 0.7290 perplexity 1.02\n",
      "global step 3800 learning rate 0.7290 perplexity 1.02\n",
      "global step 3900 learning rate 0.7290 perplexity 1.02\n",
      "global step 4000 learning rate 0.7290 perplexity 1.01\n",
      ">>>>>>>>>  the quick brown fox  ->  eekkkkknnnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 4100 learning rate 0.7290 perplexity 1.01\n",
      "global step 4200 learning rate 0.7290 perplexity 1.01\n",
      "global step 4300 learning rate 0.7290 perplexity 1.01\n",
      "global step 4400 learning rate 0.7290 perplexity 1.00\n",
      "global step 4500 learning rate 0.7290 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eekckkkknnnnnnnxxxxn\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 4600 learning rate 0.7290 perplexity 1.01\n",
      "global step 4700 learning rate 0.7290 perplexity 1.00\n",
      "global step 4800 learning rate 0.7290 perplexity 1.00\n",
      "global step 4900 learning rate 0.7290 perplexity 1.00\n",
      "global step 5000 learning rate 0.7290 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eekkkkknnnnnnnxxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 5100 learning rate 0.7290 perplexity 1.00\n",
      "global step 5200 learning rate 0.7290 perplexity 1.00\n",
      "global step 5300 learning rate 0.6561 perplexity 1.00\n",
      "global step 5400 learning rate 0.6561 perplexity 1.00\n",
      "global step 5500 learning rate 0.6561 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eekkkkknnnnnnnxxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 5600 learning rate 0.6561 perplexity 1.00\n",
      "global step 5700 learning rate 0.6561 perplexity 1.04\n",
      "global step 5800 learning rate 0.5905 perplexity 1.01\n",
      "global step 5900 learning rate 0.5905 perplexity 1.01\n",
      "global step 6000 learning rate 0.5905 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eekkkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 6100 learning rate 0.5905 perplexity 1.00\n",
      "global step 6200 learning rate 0.5905 perplexity 1.00\n",
      "global step 6300 learning rate 0.5905 perplexity 1.00\n",
      "global step 6400 learning rate 0.5905 perplexity 1.00\n",
      "global step 6500 learning rate 0.5905 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eekkkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 6600 learning rate 0.5905 perplexity 1.00\n",
      "global step 6700 learning rate 0.5905 perplexity 1.00\n",
      "global step 6800 learning rate 0.5905 perplexity 1.00\n",
      "global step 6900 learning rate 0.5905 perplexity 1.00\n",
      "global step 7000 learning rate 0.5905 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eekkkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 7100 learning rate 0.5905 perplexity 1.00\n",
      "global step 7200 learning rate 0.5905 perplexity 1.00\n",
      "global step 7300 learning rate 0.5905 perplexity 1.00\n",
      "global step 7400 learning rate 0.5905 perplexity 1.00\n",
      "global step 7500 learning rate 0.5905 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eekkkkkknnnnnnnxxxxk\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 7600 learning rate 0.5314 perplexity 1.00\n",
      "global step 7700 learning rate 0.5314 perplexity 1.00\n",
      "global step 7800 learning rate 0.4783 perplexity 1.00\n",
      "global step 7900 learning rate 0.4783 perplexity 1.00\n",
      "global step 8000 learning rate 0.4783 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eekkkkkknnnnnnnxxxxk\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 8100 learning rate 0.4783 perplexity 1.00\n",
      "global step 8200 learning rate 0.4305 perplexity 1.00\n",
      "global step 8300 learning rate 0.3874 perplexity 1.00\n",
      "global step 8400 learning rate 0.3874 perplexity 1.00\n",
      "global step 8500 learning rate 0.3874 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnxxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 8600 learning rate 0.3874 perplexity 1.00\n",
      "global step 8700 learning rate 0.3874 perplexity 1.00\n",
      "global step 8800 learning rate 0.3874 perplexity 1.00\n",
      "global step 8900 learning rate 0.3874 perplexity 1.00\n",
      "global step 9000 learning rate 0.3874 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eekkkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 9100 learning rate 0.3487 perplexity 1.00\n",
      "global step 9200 learning rate 0.3138 perplexity 1.00\n",
      "global step 9300 learning rate 0.3138 perplexity 1.00\n",
      "global step 9400 learning rate 0.3138 perplexity 1.00\n",
      "global step 9500 learning rate 0.3138 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 9600 learning rate 0.3138 perplexity 1.00\n",
      "global step 9700 learning rate 0.3138 perplexity 1.00\n",
      "global step 9800 learning rate 0.3138 perplexity 1.00\n",
      "global step 9900 learning rate 0.3138 perplexity 1.00\n",
      "global step 10000 learning rate 0.3138 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 10100 learning rate 0.2824 perplexity 1.00\n",
      "global step 10200 learning rate 0.2824 perplexity 1.00\n",
      "global step 10300 learning rate 0.2824 perplexity 1.00\n",
      "global step 10400 learning rate 0.2824 perplexity 1.00\n",
      "global step 10500 learning rate 0.2824 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 10600 learning rate 0.2542 perplexity 1.00\n",
      "global step 10700 learning rate 0.2542 perplexity 1.00\n",
      "global step 10800 learning rate 0.2542 perplexity 1.00\n",
      "global step 10900 learning rate 0.2542 perplexity 1.00\n",
      "global step 11000 learning rate 0.2542 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 11100 learning rate 0.2542 perplexity 1.00\n",
      "global step 11200 learning rate 0.2542 perplexity 1.00\n",
      "global step 11300 learning rate 0.2542 perplexity 1.00\n",
      "global step 11400 learning rate 0.2288 perplexity 1.00\n",
      "global step 11500 learning rate 0.2288 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 11600 learning rate 0.2288 perplexity 1.00\n",
      "global step 11700 learning rate 0.2288 perplexity 1.00\n",
      "global step 11800 learning rate 0.2288 perplexity 1.00\n",
      "global step 11900 learning rate 0.2288 perplexity 1.00\n",
      "global step 12000 learning rate 0.2059 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 12100 learning rate 0.2059 perplexity 1.00\n",
      "global step 12200 learning rate 0.2059 perplexity 1.00\n",
      "global step 12300 learning rate 0.2059 perplexity 1.00\n",
      "global step 12400 learning rate 0.2059 perplexity 1.00\n",
      "global step 12500 learning rate 0.1853 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eekkkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 12600 learning rate 0.1853 perplexity 1.00\n",
      "global step 12700 learning rate 0.1668 perplexity 1.00\n",
      "global step 12800 learning rate 0.1668 perplexity 1.00\n",
      "global step 12900 learning rate 0.1668 perplexity 1.00\n",
      "global step 13000 learning rate 0.1668 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eekkkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 13100 learning rate 0.1668 perplexity 1.00\n",
      "global step 13200 learning rate 0.1668 perplexity 1.00\n",
      "global step 13300 learning rate 0.1668 perplexity 1.00\n",
      "global step 13400 learning rate 0.1501 perplexity 1.00\n",
      "global step 13500 learning rate 0.1501 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 13600 learning rate 0.1351 perplexity 1.00\n",
      "global step 13700 learning rate 0.1351 perplexity 1.00\n",
      "global step 13800 learning rate 0.1216 perplexity 1.00\n",
      "global step 13900 learning rate 0.1216 perplexity 1.00\n",
      "global step 14000 learning rate 0.1216 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 14100 learning rate 0.1216 perplexity 1.00\n",
      "global step 14200 learning rate 0.1094 perplexity 1.00\n",
      "global step 14300 learning rate 0.1094 perplexity 1.00\n",
      "global step 14400 learning rate 0.1094 perplexity 1.00\n",
      "global step 14500 learning rate 0.1094 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 14600 learning rate 0.1094 perplexity 1.00\n",
      "global step 14700 learning rate 0.0985 perplexity 1.00\n",
      "global step 14800 learning rate 0.0886 perplexity 1.00\n",
      "global step 14900 learning rate 0.0798 perplexity 1.00\n",
      "global step 15000 learning rate 0.0718 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 15100 learning rate 0.0718 perplexity 1.00\n",
      "global step 15200 learning rate 0.0718 perplexity 1.00\n",
      "global step 15300 learning rate 0.0718 perplexity 1.00\n",
      "global step 15400 learning rate 0.0646 perplexity 1.00\n",
      "global step 15500 learning rate 0.0646 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 15600 learning rate 0.0581 perplexity 1.00\n",
      "global step 15700 learning rate 0.0581 perplexity 1.00\n",
      "global step 15800 learning rate 0.0581 perplexity 1.00\n",
      "global step 15900 learning rate 0.0581 perplexity 1.00\n",
      "global step 16000 learning rate 0.0581 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 16100 learning rate 0.0581 perplexity 1.00\n",
      "global step 16200 learning rate 0.0523 perplexity 1.00\n",
      "global step 16300 learning rate 0.0523 perplexity 1.00\n",
      "global step 16400 learning rate 0.0523 perplexity 1.00\n",
      "global step 16500 learning rate 0.0523 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 16600 learning rate 0.0471 perplexity 1.00\n",
      "global step 16700 learning rate 0.0471 perplexity 1.00\n",
      "global step 16800 learning rate 0.0471 perplexity 1.00\n",
      "global step 16900 learning rate 0.0471 perplexity 1.00\n",
      "global step 17000 learning rate 0.0471 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 17100 learning rate 0.0424 perplexity 1.00\n",
      "global step 17200 learning rate 0.0424 perplexity 1.00\n",
      "global step 17300 learning rate 0.0424 perplexity 1.00\n",
      "global step 17400 learning rate 0.0424 perplexity 1.00\n",
      "global step 17500 learning rate 0.0424 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 17600 learning rate 0.0424 perplexity 1.00\n",
      "global step 17700 learning rate 0.0382 perplexity 1.00\n",
      "global step 17800 learning rate 0.0382 perplexity 1.00\n",
      "global step 17900 learning rate 0.0382 perplexity 1.00\n",
      "global step 18000 learning rate 0.0382 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 18100 learning rate 0.0343 perplexity 1.00\n",
      "global step 18200 learning rate 0.0343 perplexity 1.00\n",
      "global step 18300 learning rate 0.0343 perplexity 1.00\n",
      "global step 18400 learning rate 0.0309 perplexity 1.00\n",
      "global step 18500 learning rate 0.0309 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 18600 learning rate 0.0309 perplexity 1.00\n",
      "global step 18700 learning rate 0.0309 perplexity 1.00\n",
      "global step 18800 learning rate 0.0309 perplexity 1.00\n",
      "global step 18900 learning rate 0.0278 perplexity 1.00\n",
      "global step 19000 learning rate 0.0278 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 19100 learning rate 0.0278 perplexity 1.00\n",
      "global step 19200 learning rate 0.0278 perplexity 1.00\n",
      "global step 19300 learning rate 0.0250 perplexity 1.00\n",
      "global step 19400 learning rate 0.0250 perplexity 1.00\n",
      "global step 19500 learning rate 0.0250 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 19600 learning rate 0.0250 perplexity 1.00\n",
      "global step 19700 learning rate 0.0250 perplexity 1.00\n",
      "global step 19800 learning rate 0.0225 perplexity 1.00\n",
      "global step 19900 learning rate 0.0225 perplexity 1.00\n",
      "global step 20000 learning rate 0.0225 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 20100 learning rate 0.0203 perplexity 1.00\n",
      "global step 20200 learning rate 0.0203 perplexity 1.00\n",
      "global step 20300 learning rate 0.0203 perplexity 1.00\n",
      "global step 20400 learning rate 0.0203 perplexity 1.00\n",
      "global step 20500 learning rate 0.0182 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 20600 learning rate 0.0164 perplexity 1.00\n",
      "global step 20700 learning rate 0.0164 perplexity 1.00\n",
      "global step 20800 learning rate 0.0164 perplexity 1.00\n",
      "global step 20900 learning rate 0.0164 perplexity 1.00\n",
      "global step 21000 learning rate 0.0164 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 21100 learning rate 0.0164 perplexity 1.00\n",
      "global step 21200 learning rate 0.0164 perplexity 1.00\n",
      "global step 21300 learning rate 0.0148 perplexity 1.00\n",
      "global step 21400 learning rate 0.0148 perplexity 1.00\n",
      "global step 21500 learning rate 0.0133 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 21600 learning rate 0.0133 perplexity 1.00\n",
      "global step 21700 learning rate 0.0133 perplexity 1.00\n",
      "global step 21800 learning rate 0.0133 perplexity 1.00\n",
      "global step 21900 learning rate 0.0133 perplexity 1.00\n",
      "global step 22000 learning rate 0.0120 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 22100 learning rate 0.0120 perplexity 1.00\n",
      "global step 22200 learning rate 0.0120 perplexity 1.00\n",
      "global step 22300 learning rate 0.0108 perplexity 1.00\n",
      "global step 22400 learning rate 0.0097 perplexity 1.00\n",
      "global step 22500 learning rate 0.0097 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 22600 learning rate 0.0097 perplexity 1.00\n",
      "global step 22700 learning rate 0.0097 perplexity 1.00\n",
      "global step 22800 learning rate 0.0097 perplexity 1.00\n",
      "global step 22900 learning rate 0.0087 perplexity 1.00\n",
      "global step 23000 learning rate 0.0087 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 23100 learning rate 0.0087 perplexity 1.00\n",
      "global step 23200 learning rate 0.0087 perplexity 1.00\n",
      "global step 23300 learning rate 0.0087 perplexity 1.00\n",
      "global step 23400 learning rate 0.0087 perplexity 1.00\n",
      "global step 23500 learning rate 0.0079 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 23600 learning rate 0.0079 perplexity 1.00\n",
      "global step 23700 learning rate 0.0079 perplexity 1.00\n",
      "global step 23800 learning rate 0.0079 perplexity 1.00\n",
      "global step 23900 learning rate 0.0071 perplexity 1.00\n",
      "global step 24000 learning rate 0.0071 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 24100 learning rate 0.0071 perplexity 1.00\n",
      "global step 24200 learning rate 0.0071 perplexity 1.00\n",
      "global step 24300 learning rate 0.0071 perplexity 1.00\n",
      "global step 24400 learning rate 0.0071 perplexity 1.00\n",
      "global step 24500 learning rate 0.0064 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 24600 learning rate 0.0064 perplexity 1.00\n",
      "global step 24700 learning rate 0.0064 perplexity 1.00\n",
      "global step 24800 learning rate 0.0064 perplexity 1.00\n",
      "global step 24900 learning rate 0.0057 perplexity 1.00\n",
      "global step 25000 learning rate 0.0052 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 25100 learning rate 0.0052 perplexity 1.00\n",
      "global step 25200 learning rate 0.0046 perplexity 1.00\n",
      "global step 25300 learning rate 0.0046 perplexity 1.00\n",
      "global step 25400 learning rate 0.0046 perplexity 1.00\n",
      "global step 25500 learning rate 0.0046 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 25600 learning rate 0.0042 perplexity 1.00\n",
      "global step 25700 learning rate 0.0042 perplexity 1.00\n",
      "global step 25800 learning rate 0.0042 perplexity 1.00\n",
      "global step 25900 learning rate 0.0042 perplexity 1.00\n",
      "global step 26000 learning rate 0.0042 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 26100 learning rate 0.0038 perplexity 1.00\n",
      "global step 26200 learning rate 0.0038 perplexity 1.00\n",
      "global step 26300 learning rate 0.0038 perplexity 1.00\n",
      "global step 26400 learning rate 0.0038 perplexity 1.00\n",
      "global step 26500 learning rate 0.0038 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 26600 learning rate 0.0034 perplexity 1.00\n",
      "global step 26700 learning rate 0.0034 perplexity 1.00\n",
      "global step 26800 learning rate 0.0034 perplexity 1.00\n",
      "global step 26900 learning rate 0.0030 perplexity 1.00\n",
      "global step 27000 learning rate 0.0027 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 27100 learning rate 0.0027 perplexity 1.00\n",
      "global step 27200 learning rate 0.0027 perplexity 1.00\n",
      "global step 27300 learning rate 0.0027 perplexity 1.00\n",
      "global step 27400 learning rate 0.0027 perplexity 1.00\n",
      "global step 27500 learning rate 0.0027 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 27600 learning rate 0.0027 perplexity 1.00\n",
      "global step 27700 learning rate 0.0025 perplexity 1.00\n",
      "global step 27800 learning rate 0.0025 perplexity 1.00\n",
      "global step 27900 learning rate 0.0022 perplexity 1.00\n",
      "global step 28000 learning rate 0.0022 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 28100 learning rate 0.0020 perplexity 1.00\n",
      "global step 28200 learning rate 0.0020 perplexity 1.00\n",
      "global step 28300 learning rate 0.0020 perplexity 1.00\n",
      "global step 28400 learning rate 0.0020 perplexity 1.00\n",
      "global step 28500 learning rate 0.0018 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 28600 learning rate 0.0018 perplexity 1.00\n",
      "global step 28700 learning rate 0.0018 perplexity 1.00\n",
      "global step 28800 learning rate 0.0018 perplexity 1.00\n",
      "global step 28900 learning rate 0.0018 perplexity 1.00\n",
      "global step 29000 learning rate 0.0016 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 29100 learning rate 0.0016 perplexity 1.00\n",
      "global step 29200 learning rate 0.0016 perplexity 1.00\n",
      "global step 29300 learning rate 0.0016 perplexity 1.00\n",
      "global step 29400 learning rate 0.0016 perplexity 1.00\n",
      "global step 29500 learning rate 0.0016 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "global step 29600 learning rate 0.0016 perplexity 1.00\n",
      "global step 29700 learning rate 0.0016 perplexity 1.00\n",
      "global step 29800 learning rate 0.0016 perplexity 1.00\n",
      "global step 29900 learning rate 0.0016 perplexity 1.00\n",
      "global step 30000 learning rate 0.0015 perplexity 1.00\n",
      ">>>>>>>>>  the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n",
      "  valid eval:  perplexity 1.00\n",
      "## :  [5, 5, 5, 11, 11, 11, 11, 11, 14, 14, 14, 14, 14, 14, 14, 24, 24, 24, 24, 24]\n",
      "Unexpected character: !\n",
      "the quick brown fox  ->  eeekkkkknnnnnnnxxxxx\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model = create_model(False)\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    num_steps = 30001\n",
    "\n",
    "    # This is the training loop.\n",
    "    step_time, loss = 0.0, 0.0\n",
    "    current_step = 0\n",
    "    previous_losses = []\n",
    "    step_ckpt = 100\n",
    "    valid_ckpt = 500\n",
    "\n",
    "    for step in range(1, num_steps):\n",
    "        model.batch_size = batch_size\n",
    "        batches = train_batches.next()\n",
    "        train_sets = []\n",
    "        batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "        batch_decs = list(map(lambda x: rev_id(x), batches))\n",
    "        for i in range(len(batch_encs)):\n",
    "            train_sets.append((batch_encs[i], batch_decs[i]))\n",
    "\n",
    "        # Get a batch and make a step.\n",
    "        encoder_inputs, decoder_inputs, target_weights = model.get_batch([train_sets], 0)\n",
    "        _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, False)\n",
    "\n",
    "        loss += step_loss / step_ckpt\n",
    "\n",
    "        # Once in a while, we save checkpoint, print statistics, and run evals.\n",
    "        if step % step_ckpt == 0:\n",
    "            # Print statistics for the previous epoch.\n",
    "            perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "            print(\"global step %d learning rate %.4f perplexity \"\n",
    "                  \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(), perplexity))\n",
    "            # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "            if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "                sess.run(model.learning_rate_decay_op)\n",
    "            previous_losses.append(loss)\n",
    "\n",
    "            loss = 0.0\n",
    "\n",
    "            if step % valid_ckpt == 0:\n",
    "                v_loss = 0.0\n",
    "\n",
    "                model.batch_size = 1\n",
    "                batches = ['the quick brown fox']\n",
    "                test_sets = []\n",
    "                batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "                # batch_decs = map(lambda x: rev_id(x), batches)\n",
    "                test_sets.append((batch_encs[0], []))\n",
    "                # Get a 1-element batch to feed the sentence to the model.\n",
    "                encoder_inputs, decoder_inputs, target_weights = model.get_batch([test_sets], 0)\n",
    "                # Get output logits for the sentence.\n",
    "                _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "\n",
    "                # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "                outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "\n",
    "                print('>>>>>>>>> ', batches[0], ' -> ', ''.join(map(lambda x: id2char(x), outputs)))\n",
    "\n",
    "                for _ in range(valid_size):\n",
    "                    model.batch_size = 1\n",
    "                    v_batches = valid_batches.next()\n",
    "                    valid_sets = []\n",
    "                    v_batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), v_batches))\n",
    "                    v_batch_decs = list(map(lambda x: rev_id(x), v_batches))\n",
    "                    for i in range(len(v_batch_encs)):\n",
    "                        valid_sets.append((v_batch_encs[i], v_batch_decs[i]))\n",
    "                    encoder_inputs, decoder_inputs, target_weights = model.get_batch([valid_sets], 0)\n",
    "                    _, eval_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "                    v_loss += eval_loss / valid_size\n",
    "\n",
    "                eval_ppx = math.exp(v_loss) if v_loss < 300 else float('inf')\n",
    "                print(\"  valid eval:  perplexity %.2f\" % (eval_ppx))\n",
    "\n",
    "    # reuse variable -> subdivide into two boxes\n",
    "    model.batch_size = 1  # We decode one sentence at a time.\n",
    "    batches = ['the quick brown fox']\n",
    "    test_sets = []\n",
    "    batch_encs = list(map(lambda x: list(map(lambda y: char2id(y), list(x))), batches))\n",
    "    # batch_decs = map(lambda x: rev_id(x), batches)\n",
    "    test_sets.append((batch_encs[0], []))\n",
    "    # Get a 1-element batch to feed the sentence to the model.\n",
    "    encoder_inputs, decoder_inputs, target_weights = model.get_batch([test_sets], 0)\n",
    "    # Get output logits for the sentence.\n",
    "    _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, 0, True)\n",
    "    # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "    outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "    print('## : ', outputs)\n",
    "    # If there is an EOS symbol in outputs, cut them at that point.\n",
    "    if char2id('!') in outputs:\n",
    "        outputs = outputs[:outputs.index(char2id('!'))]\n",
    "\n",
    "    print(batches[0], ' -> ', ''.join(map(lambda x: id2char(x), outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
